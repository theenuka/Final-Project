# Ansible Automation for Phoenix Kubernetes Cluster

## Overview
This Ansible configuration automates the deployment of a production-ready Kubeadm cluster on AWS with:
- **AWS Cloud Controller Manager (CCM)** - Native AWS integration
- **Calico CNI** - Network plugin
- **External Cloud Provider Mode** - Required for AWS Load Balancer Controller and EBS CSI Driver

## Architecture

### Roles

#### 1. Common Role (`roles/common`)
Configures all nodes with:
- **Hostname**: Set to EC2 Private DNS (critical for CCM node matching)
- **Container Runtime**: containerd with systemd cgroup driver
- **Kubernetes Components**: kubelet, kubeadm, kubectl (v1.28)
- **Kubelet Configuration**: `--cloud-provider=external` flag
- **Kernel Parameters**: Enable IP forwarding and bridge netfilter
- **Swap**: Disabled (required by Kubernetes)

#### 2. Master Role (`roles/master`)
Initializes the control plane:
- **kubeadm init**: With `--cloud-provider=external` (no cloud-provider flag to kubeadm)
- **Calico CNI**: Pod network (192.168.0.0/16)
- **AWS Cloud Controller Manager**: CRITICAL - Removes node taints and enables Ready state
- **Join Token**: Generated for worker nodes

#### 3. Worker Role (`roles/worker`)
Joins worker nodes:
- Executes join command from master
- Waits for kubelet to be active
- CCM will automatically untaint and mark as Ready

## Prerequisites

1. **Terraform**: Infrastructure must be deployed first
2. **Inventory File**: Auto-generated by Terraform at `inventory.ini`
3. **SSH Access**: Private key at `~/.ssh/id_rsa`
4. **Ansible**: Version >= 2.10

## Usage

### 1. Deploy Infrastructure First
```bash
cd ../terraform
terraform apply
```

This generates `inventory.ini` automatically.

### 2. Verify Inventory
```bash
cd ../ansible
cat inventory.ini
```

Should show:
- 1 master node
- 2 worker nodes
- All with correct IPs and private DNS

### 3. Test Connectivity
```bash
ansible all -m ping
```

### 4. Deploy Kubernetes Cluster
```bash
ansible-playbook site.yml
```

This will:
1. Configure all nodes (15-20 minutes)
2. Initialize master with CCM
3. Join worker nodes
4. Wait for all nodes to be Ready

### 5. Verify Cluster
```bash
# SSH to master node
ssh ubuntu@<master-ip>

# Check nodes (all should be Ready)
kubectl get nodes

# Check CCM is running
kubectl get pods -n kube-system -l k8s-app=aws-cloud-controller-manager

# Verify nodes have AWS provider ID
kubectl get nodes -o wide
```

## Important Notes

### Cloud Provider External Mode
- Nodes start with `node.cloudprovider.kubernetes.io/uninitialized:NoSchedule` taint
- AWS CCM removes this taint after successful registration
- **Without CCM, nodes will remain NotReady forever**

### Hostname Configuration
- Nodes MUST use EC2 Private DNS as hostname
- CCM matches nodes to EC2 instances by hostname
- Format: `ip-10-0-X-X.ec2.internal`

### Kubelet Configuration
The `/etc/default/kubelet` file contains:
```bash
KUBELET_EXTRA_ARGS="--cloud-provider=external"
```

This is REQUIRED for AWS cloud controllers to function.

### AWS Cloud Controller Manager
The CCM manifest is applied immediately after cluster initialization. It:
- Runs on the master node only
- Tolerates control-plane taints
- Sets `providerID` on nodes (e.g., `aws:///us-east-1a/i-xxxxx`)
- Removes initialization taints
- Enables AWS-specific features (ELB, EBS integration)

## Post-Installation

After cluster is ready, install additional controllers:

### AWS Load Balancer Controller
```bash
# Apply from master node
kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller/crds?ref=master"

# Install via Helm
helm repo add eks https://aws.github.io/eks-charts
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=phoenix-cluster \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller
```

### EBS CSI Driver
```bash
# Install via Helm
helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver
helm install aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver \
  -n kube-system \
  --set enableVolumeScheduling=true \
  --set enableVolumeResizing=true \
  --set enableVolumeSnapshot=true
```

## Troubleshooting

### Nodes Stuck in NotReady
```bash
# Check CCM logs
kubectl logs -n kube-system -l k8s-app=aws-cloud-controller-manager

# Common issues:
# 1. IAM permissions missing
# 2. Hostname doesn't match EC2 Private DNS
# 3. Security groups blocking API access
```

### CCM Not Starting
```bash
# Check if master has correct labels
kubectl get nodes --show-labels | grep control-plane

# Verify tolerations
kubectl describe ds -n kube-system aws-cloud-controller-manager
```

### Worker Nodes Not Joining
```bash
# Check join token on master
kubeadm token list

# Regenerate if expired
kubeadm token create --print-join-command

# Check kubelet logs on worker
journalctl -u kubelet -f
```

## Files Structure

```
ansible/
├── ansible.cfg              # Ansible configuration
├── site.yml                 # Main playbook
├── inventory.ini            # Auto-generated by Terraform
├── roles/
│   ├── common/
│   │   └── tasks/
│   │       └── main.yml     # Base setup for all nodes
│   ├── master/
│   │   ├── tasks/
│   │   │   └── main.yml     # Master initialization + CCM
│   │   └── files/
│   │       └── aws-ccm-daemonset-custom.yaml
│   └── worker/
│       └── tasks/
│           └── main.yml     # Worker join logic
└── README.md                # This file
```

## Validation Checklist

After deployment, verify:

- [ ] All nodes show `Ready` status
- [ ] AWS CCM pod is running in kube-system
- [ ] Calico pods are running (one per node)
- [ ] Nodes have `providerID` set (check with `kubectl get nodes -o yaml`)
- [ ] Hostname matches EC2 Private DNS
- [ ] Kubelet running with `--cloud-provider=external`
- [ ] No `uninitialized` taints on nodes

## Next Steps

1. **Install AWS Load Balancer Controller** - For Ingress/Service LoadBalancers
2. **Install EBS CSI Driver** - For persistent volumes
3. **Deploy applications** - Test with LoadBalancer service and PVC
4. **Set up monitoring** - Prometheus, Grafana
5. **Configure RBAC** - Create users and service accounts

## Support

For issues:
1. Check Ansible output for errors
2. Verify IAM permissions on EC2 instances
3. Check security groups allow required ports
4. Ensure VPC/subnet tags are correct
5. Review kubelet and CCM logs

## References

- [Kubernetes Cloud Provider AWS](https://github.com/kubernetes/cloud-provider-aws)
- [AWS Load Balancer Controller](https://kubernetes-sigs.github.io/aws-load-balancer-controller/)
- [EBS CSI Driver](https://github.com/kubernetes-sigs/aws-ebs-csi-driver)
- [Calico Documentation](https://docs.projectcalico.org/)
